version: "3.8"
services:
  sql-client:
    build: ./docker-images/flink
    container_name: 'sql-client'
    restart: always
    command: 
      - /bin/sh
      - -c
      - |
        cp /host/conf/flink/sql-config.yaml /opt/flink/conf/config.yaml
        cp /host/conf/flink/hive-site.xml /opt/flink/conf
        sed -i 's/AWS_ACCESS_KEY_ID/'$AWS_ACCESS_KEY_ID'/g' /opt/flink/conf/hive-site.xml
        sed -i 's/AWS_SECRET_ACCESS_KEY/'$AWS_SECRET_ACCESS_KEY'/g' /opt/flink/conf/hive-site.xml
        #bin/sql-client.sh
        echo "Starting the SQL Client instance."
        sleep infinity
    environment:
      - AWS_REGION=${AWS_REGION}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    volumes:
      - ./data/:/data
      - ./jar-packs:/jar-packs
      - ./:/host

  jobmanager:
    container_name: 'jobmanager'
    build: ./docker-images/flink
    restart: always
    ports:
      - 8081:8081
      - 6123:6123
    command: 
      - /bin/sh
      - -c
      - |
        cp /host/conf/flink/job-config.yaml /opt/flink/conf/config.yaml
        cp /host/conf/flink/hive-site.xml /opt/flink/conf
        sed -i 's/AWS_ACCESS_KEY_ID/'$AWS_ACCESS_KEY_ID'/g' /opt/flink/conf/hive-site.xml
        sed -i 's/AWS_SECRET_ACCESS_KEY/'$AWS_SECRET_ACCESS_KEY'/g' /opt/flink/conf/hive-site.xml
        mkdir -p /data/catalog_store
        mkdir -p /data/rocksdb/checkpoint
        mkdir -p /data/rocksdb/data
        mkdir -p /data/recovery
        /docker-entrypoint.sh jobmanager
    environment:
      - AWS_REGION=${AWS_REGION}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    volumes:
      - ./jar-packs:/jar-packs
      - ./:/host
      - ./data/:/data

  taskmanager:
    build: ./docker-images/flink
    restart: always
    depends_on:
      - jobmanager
    command: 
      - /bin/sh
      - -c
      - |
        cp /host/conf/flink/task-config.yaml /opt/flink/conf/config.yaml
        cp /host/conf/flink/hive-site.xml /opt/flink/conf
        sed -i 's/AWS_ACCESS_KEY_ID/'$AWS_ACCESS_KEY_ID'/g' /opt/flink/conf/hive-site.xml
        sed -i 's/AWS_SECRET_ACCESS_KEY/'$AWS_SECRET_ACCESS_KEY'/g' /opt/flink/conf/hive-site.xml
        /docker-entrypoint.sh taskmanager
    scale: 1
    environment:
      - AWS_REGION=${AWS_REGION}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    volumes:
      - ./jar-packs:/jar-packs
      - ./:/host
      - ./data/:/data

  zk:
    image: zookeeper:3.7.0
    restart: always
    container_name: zk
    hostname: zk
    ports:
      - 2181:2181
      - 7001:7000
    volumes:
      - zookeeper_log-volume:/logs
      - zookeeper_data-volume:/data
      - zookeeper_datalog-volume:/datalog
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    deploy:
      placement:
        constraints: 
          - "node.role == manager"
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: server.1=zk:2888:3888;2181 
      ZOO_4LW_COMMANDS_WHITELIST: mntr, conf, ruok

  minio:
    image: minio/minio
    container_name: minio
    restart: always
    environment:
      - MINIO_ROOT_USER=${AWS_ACCESS_KEY_ID}
      - MINIO_ROOT_PASSWORD=${AWS_SECRET_ACCESS_KEY}
    ports:
      - 9001:9001
      - 9000:9000
    command: ["server", "/data", "--console-address", ":9001"]
    volumes: 
      - minio-data-volume:/data

  mc:
    depends_on:
      - minio
    image: minio/mc
    container_name: mc
    entrypoint: >
      /bin/sh -c "
      until (/usr/bin/mc config host add minio http://minio:9000 ${AWS_ACCESS_KEY_ID} ${AWS_SECRET_ACCESS_KEY}) do echo '...waiting...' && sleep 1; done;
      #/usr/bin/mc rm -r --force minio/warehouse;
      /usr/bin/mc mb minio/warehouse;
      tail -f /dev/null
      " 

  hive-metastore:
    container_name: hms
    restart: always
    build: ./docker-images/hive
    ports:
      - 9083:9083
    environment:
      - HMS_LOGLEVEL=INFO
      - SERVICE_NAME=metastore
      - AWS_REGION=${AWS_REGION}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    entrypoint: >
      /bin/sh -c "echo 'Copying metastore configuration...' && cp /host/conf/hive/metastore-site.xml /opt/hive/conf/; 
      sed -i 's/AWS_ACCESS_KEY_ID/'$AWS_ACCESS_KEY_ID'/g' /opt/hive/conf/metastore-site.xml; 
      sed -i 's/AWS_SECRET_ACCESS_KEY/'$AWS_SECRET_ACCESS_KEY'/g' /opt/hive/conf/metastore-site.xml; 
      /entrypoint.sh"
    volumes:
      - ./:/host
      - ./data/:/data

# Without a network explicitly defined, you hit this Hive/Thrift error
# java.net.URISyntaxException Illegal character in hostname
# https://github.com/TrivadisPF/platys-modern-data-platform/issues/231
networks:
  default:
     name: flink-stack

volumes:
  zookeeper_log-volume:
  zookeeper_data-volume:
  zookeeper_datalog-volume:
  minio-data-volume:
